{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the dataset from Kaggle Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kelley\\AppData\\Local\\Programs\\Microsoft VS Code\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\n\\nestablishment_df=pd.read_csv(\"data/Grab SG Restaurants.csv\")\\n\\nestablishment_df.head()\\n\\n#Used by Hassan\\ncuisine_df=establishment_df.copy()\\n\\n# Used my Lee Hong Eng\\ngrab_df = establishment_df.copy()\\n\\n# Used by Rosni\\ngrab_rest = establishment_df.copy()\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "establishment_df=pd.read_csv(\"data/Grab SG Restaurants.csv\")\n",
    "\n",
    "establishment_df.head()\n",
    "\n",
    "#Used by Hassan\n",
    "cuisine_df=establishment_df.copy()\n",
    "\n",
    "# Used my Lee Hong Eng\n",
    "grab_df = establishment_df.copy()\n",
    "\n",
    "# Used by Rosni\n",
    "grab_rest = establishment_df.copy()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Simple exploratory Data Analysis to understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "establishment_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the dataset of 16136 rows of grab restaurants\n",
    "you can see from the count that there are empty values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "establishment_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning on 'address' column: SAILA\n",
    "### Which involves data inspection, Data correction, Data standardization and Data transformation.\n",
    "\n",
    "- Remove duplicates-> the address column values are keyed in together with the establishment name seperated by \"-\"\n",
    "- Standardize the wording -> currently the values entries are inconsistent cases: \n",
    "    example : 'Vivocity' , 'VivoCity'\n",
    "- Inconsistent entry -> for the same address it was entered inconsistantly\n",
    "    example: 'Vivocity', 'Vivo City' , 'Vivocity Shopping mall'\n",
    "- Need to filter and transform the address entries to avoid unnecessary disticnt values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data .Removing values before '-'\n",
    "establishment_df['address'] = establishment_df['address'].apply(lambda x: x.split('-')[-1].strip())\n",
    "\n",
    "# Display the modified DataFrame\n",
    "print(establishment_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To Understand the dataframe entries-Distinct values at address column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "distinct_addresses = establishment_df['address'].unique()\n",
    "address_counts = establishment_df['address'].value_counts()\n",
    "\n",
    "# Display distinct values and their counts\n",
    "print(\"Distinct Addresses:\")\n",
    "print(distinct_addresses)\n",
    "print(\"\\nCount of Distinct Addresses:\")\n",
    "print(address_counts.head(50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Distinct values count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'establishment_df' is your DataFrame containing the data\n",
    "total_distinct_addresses = establishment_df['address'].nunique()\n",
    "\n",
    "# Display the total number of distinct addresses\n",
    "print(\"Total distinct addresses:\", total_distinct_addresses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize all the add with tittle case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'establishment_df' is your DataFrame containing the data\n",
    "establishment_df['address'] = establishment_df['address'].apply(lambda x: x.title())\n",
    "\n",
    "# Display the modified DataFrame\n",
    "establishment_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the [address] column further by changing the case to tittle form.The distinct count reduced from 4998 to 4883"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To find the total distinct value after changing the values in address column to tittle case.\n",
    "total_distinct_addresses = establishment_df['address'].nunique()\n",
    "\n",
    "# Display the total number of distinct addresses\n",
    "print(\"Total distinct addresses:\", total_distinct_addresses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out the distinct data using alphabets to verify the values in the column and to do necessary transformation needed if the data was not input in accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'establishment_df' is your DataFrame containing the data\n",
    "\n",
    "# Step 1: Filter the DataFrame\n",
    "filtered_df = establishment_df[establishment_df['address'].str.lower().str.startswith(('a'))]\n",
    "\n",
    "# Step 2: Extract distinct values from the filtered DataFrame\n",
    "distinct_addresses_filtered = filtered_df['address'].unique()\n",
    "\n",
    "# Step 3: Sort the distinct values alphabetically\n",
    "distinct_addresses_filtered_sorted = sorted(distinct_addresses_filtered)\n",
    "\n",
    "# Display the sorted distinct values\n",
    "print(\"Distinct addresses starting with 'a':\")\n",
    "for address in distinct_addresses_filtered_sorted:\n",
    "    print(address)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tried using difflib library to standardize the entries up to 80% matching.( Didn't work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "# Filter the DataFrame to include only addresses starting with 'a'\n",
    "addresses_starting_with_a = establishment_df[establishment_df['address'].str.lower().str.startswith('a')]['address'].unique()\n",
    "\n",
    "# Define a function to find the best match for each address starting with 'a'\n",
    "def standardize_address(address):\n",
    "    matches = difflib.get_close_matches(address, addresses_starting_with_a, n=1, cutoff=0.7)\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    else:\n",
    "        return address\n",
    "\n",
    "# Apply the standardization function to addresses starting with 'a'\n",
    "standardized_addresses_starting_with_a = [standardize_address(address) for address in addresses_starting_with_a]\n",
    "\n",
    "# Display the original and standardized addresses\n",
    "print(\"Original Addresses:\")\n",
    "print(addresses_starting_with_a)\n",
    "\n",
    "print(\"\\nStandardized Addresses:\")\n",
    "print(standardized_addresses_starting_with_a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The result shows the total number of entries are the same before and after 'standardization' using the 'difflib' library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of entries before standardization\n",
    "total_entries_before = len(addresses_starting_with_a)\n",
    "\n",
    "# Count the total number of entries after standardization\n",
    "total_entries_after = len(set(standardized_addresses_starting_with_a))\n",
    "\n",
    "# Print the total number of entries before and after standardization\n",
    "print(\"Total entries with addresses starting with 'a' before standardization:\", total_entries_before)\n",
    "print(\"Total entries with addresses starting with 'a' after standardization:\", total_entries_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize all the entries that have '[xxx]' and replace all the 'And' with '&' for standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define a function to standardize addresses and count changes\n",
    "def standardize_address_and_count(address):\n",
    "    # Initialize counter for changes\n",
    "    num_changes = 0\n",
    "    \n",
    "    # Remove '[Stall xx]' part using regular expression\n",
    "    standardized_address = re.sub(r'\\[.*?\\]', '', address)\n",
    "    \n",
    "    # Replace 'And' with '&' and remove leading/trailing spaces\n",
    "    standardized_address = standardized_address.replace('And', '&').strip()\n",
    "    \n",
    "    # Replace any occurrence of 'Abc Brickworks' with 'Abc Brickworks Market & Food Centre'\n",
    "    if 'Abc Brickworks' in standardized_address:\n",
    "        standardized_address = standardized_address.replace('Abc Brickworks', 'Abc Brickworks Market & Food Centre')\n",
    "        num_changes += 1\n",
    "    \n",
    "    return standardized_address, num_changes\n",
    "\n",
    "# Apply the standardization function to the 'address' column of the DataFrame\n",
    "establishment_df['standardized_address'], num_changes = zip(*establishment_df['address'].apply(standardize_address_and_count))\n",
    "\n",
    "# Print the total count of entries that were changed\n",
    "print(\"Total count of entries changed:\", sum(num_changes))\n",
    "\n",
    "# Print the list of entries that were changed\n",
    "changed_entries = establishment_df[establishment_df['standardized_address'] != establishment_df['address']]\n",
    "print(\"\\nList of entries that were changed:\")\n",
    "for index, row in changed_entries.iterrows():\n",
    "    print(row['address'], '->', row['standardized_address'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To find the total distinct value after changing the values in address column to tittle case.\n",
    "total_distinct_addresses = establishment_df['address'].nunique()\n",
    "\n",
    "# Display the total number of distinct addresses\n",
    "print(\"Total distinct addresses:\", total_distinct_addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter the DataFrame\n",
    "filtered_df = establishment_df[establishment_df['address'].str.lower().str.startswith(('a'))]\n",
    "\n",
    "# Step 2: Extract distinct values from the filtered DataFrame\n",
    "distinct_addresses_filtered = filtered_df['address'].unique()\n",
    "\n",
    "# Step 3: Sort the distinct values alphabetically\n",
    "distinct_addresses_filtered_sorted = sorted(distinct_addresses_filtered)\n",
    "\n",
    "# Display the sorted distinct values\n",
    "print(\"Distinct addresses starting with 'a':\")\n",
    "for address in distinct_addresses_filtered_sorted:\n",
    "    print(address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2nd attempt on data standardization (it created a seperate columns instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define a function to standardize addresses and count changes\n",
    "def standardize_address_and_count(address):\n",
    "    # Initialize counter for changes\n",
    "    num_changes = 0\n",
    "    \n",
    "    # Remove '[Stall xx]' part using regular expression\n",
    "    standardized_address = re.sub(r'\\[.*?\\]', '', address)\n",
    "    \n",
    "    # Replace 'And' with '&' and remove leading/trailing spaces\n",
    "    standardized_address = standardized_address.replace('And', '&').strip()\n",
    "    \n",
    "    # Replace any occurrence of 'Abc Brickworks' with 'Abc Brickworks Market & Food Centre'\n",
    "    if 'Abc Brickworks' in standardized_address:\n",
    "        standardized_address = standardized_address.replace('Abc Brickworks', 'Abc Brickworks Market & Food Centre')\n",
    "        num_changes += 1\n",
    "    \n",
    "    return standardized_address, num_changes\n",
    "\n",
    "# Apply the standardization function to the 'address' column of the DataFrame\n",
    "establishment_df['address'], num_changes = zip(*establishment_df['address'].apply(standardize_address_and_count))\n",
    "\n",
    "# Print the total count of entries that were changed\n",
    "print(\"Total count of entries changed:\", sum(num_changes))\n",
    "\n",
    "# Print the list of entries that were changed\n",
    "changed_entries = establishment_df[establishment_df['standardized_address'] != establishment_df['address']]\n",
    "\n",
    "print(\"\\nList of entries that were changed:\")\n",
    "for index, row in changed_entries.iterrows():\n",
    "    print(row['address'], '->', row['standardized_address'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter the DataFrame\n",
    "filtered_df = establishment_df[establishment_df['address'].str.lower().str.startswith(('a'))]\n",
    "\n",
    "# Step 2: Extract distinct values from the filtered DataFrame\n",
    "distinct_addresses_filtered = filtered_df['address'].unique()\n",
    "\n",
    "# Step 3: Sort the distinct values alphabetically\n",
    "distinct_addresses_filtered_sorted = sorted(distinct_addresses_filtered)\n",
    "\n",
    "# Display the sorted distinct values\n",
    "print(\"Distinct addresses starting with 'a':\")\n",
    "for address in distinct_addresses_filtered_sorted:\n",
    "    print(address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The distinct Values reduced from 4883 to 4029"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To find the total distinct value after standardizing the data with removal of '[xxx]' and change 'And' to '&'\n",
    "total_distinct_addresses = establishment_df['address'].nunique()\n",
    "\n",
    "# Display the total number of distinct addresses\n",
    "print(\"Total distinct addresses:\", total_distinct_addresses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further standiardize the entries in address column by removing whatever additional information after the add like brackets and such, replace all the 'Amk' to 'Ang Mo Kio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define a function to standardize addresses\n",
    "def standardize_address(address):\n",
    "    # Condition 1: Remove additional information after the address\n",
    "    address = re.sub(r'\\s*\\([^)]*\\)', '', address)\n",
    "\n",
    "    # Condition 2: Replace variations of 'Ayer Rajah Food Centre' with 'Ayer Rajah Food Centre'\n",
    "    if 'Ayer Rajah Food Centre' in address:\n",
    "        address = 'Ayer Rajah Food Centre'\n",
    "\n",
    "    # Condition 3: Replace variations of 'Ayer Rajah CC' with 'Ayer Rajah CC'\n",
    "    elif 'Ayer Rajah Cc' in address or 'Ayer Rajah Community Centre' in address:\n",
    "        address = 'Ayer Rajah CC'\n",
    "\n",
    "    # Condition 4: Replace variations of 'Asia Square' with 'Asia Square'\n",
    "    elif 'Asia Square' in address:\n",
    "        address = 'Asia Square'\n",
    "\n",
    "    # Replace variation 'Abc Brickworks Market & Food Centre Market & Food Centre Food Centre' \n",
    "    elif 'Abc Brickworks Market & Food Centre' in address:\n",
    "        address = 'Abc Brickworks Market & Food Centre'\n",
    "\n",
    "        \n",
    "    # Condition 5: Replace 'Amk' with 'Ang Mo Kio'\n",
    "    address = address.replace('Amk', 'Ang Mo Kio')\n",
    "\n",
    "    #Replace variation 'Abc Brickworks Market & Food Centre Market & Food Centre Food Centre' \n",
    "    \n",
    "    return address\n",
    "\n",
    "# Apply the standardization function to the 'address' column of the DataFrame\n",
    "establishment_df['address'] = establishment_df['address'].apply(standardize_address)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter the DataFrame\n",
    "filtered_df = establishment_df[establishment_df['address'].str.lower().str.startswith(('a'))]\n",
    "\n",
    "# Step 2: Extract distinct values from the filtered DataFrame\n",
    "distinct_addresses_filtered = filtered_df['address'].unique()\n",
    "\n",
    "# Step 3: Sort the distinct values alphabetically\n",
    "distinct_addresses_filtered_sorted = sorted(distinct_addresses_filtered)\n",
    "\n",
    "# Display the sorted distinct values\n",
    "print(\"Distinct addresses starting with 'a':\")\n",
    "for address in distinct_addresses_filtered_sorted:\n",
    "    print(address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The distint values reduced further from original 4883 to 3971"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To find the total distinct value after standardizing the data with removal of '[xxx]' and change 'And' to '&'\n",
    "total_distinct_addresses = establishment_df['address'].nunique()\n",
    "\n",
    "# Display the total number of distinct addresses\n",
    "print(\"Total distinct addresses:\", total_distinct_addresses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the standardization suggestion upon further investigation as its only done base on alphabet 'a', in future more cleaning needed to standardize the address to have better grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(establishment_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out corrupt Data . Removing rows with Null values in 'name' column \n",
    "- Possible reasons as the name shows test this might be test vendors at the initial grab phrase so dropping this rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with null values in the 'name' column\n",
    "establishment_df.dropna(subset=['name'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(establishment_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "establishment_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to select rows with null values in the 'rating' column\n",
    "establishment_df[establishment_df['rating'].isnull()].head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extract distinct values from the filtered DataFrame\n",
    "establishment_df['loc_type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns to remove duplicate columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = ['country', 'cuisine', 'currency', 'delivery_cost', 'opening_hours', 'image_url', 'radius', 'delivery_options', 'promo', 'delivery_by', 'delivery_time']\n",
    "\n",
    "# Drop the specified columns from the DataFrame\n",
    "establishment_df.drop(columns=columns_to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "establishment_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noted there  was 2 columns for address: 'address' & 'standardized_address' \n",
    "- 'standardized_address' column was created as a result of trial and error in the process of data standardization\n",
    "-  find the distict values of the both columns to chk if the standardization applied correctly to 'address' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To find the total distinct value after standardizing the data with removal of '[xxx]' and change 'And' to '&'\n",
    "total_distinct_addresses = establishment_df['standardized_address' ].nunique()\n",
    "\n",
    "# Display the total number of distinct addresses\n",
    "print(\"Total distinct addresses:\", total_distinct_addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To find the total distinct value after standardizing the data with removal of '[xxx]' and change 'And' to '&'\n",
    "total_distinct_addresses = establishment_df['address'].nunique()\n",
    "\n",
    "# Display the total number of distinct addresses\n",
    "print(\"Total distinct addresses:\", total_distinct_addresses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop another column that was created as a result of trial and error in the process of data standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = ['standardized_address']\n",
    "\n",
    "# Drop the specified columns from the DataFrame\n",
    "establishment_df.drop(columns=columns_to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "establishment_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change all entries under 'address' column to lowercase to standardized further for easier analytics process in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the values in the 'address' column to lowercase\n",
    "establishment_df['address'] = establishment_df['address'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "establishment_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download neccesay libraries to Load the  establishment dataframe to postgres database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sqlalchemy\n",
    "!pip install psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding to a Database Grab_df : ESTABLISHMENT_TBL - SAILA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy import create_engine\n",
    "# username='postgres'\n",
    "# password = 'admin'\n",
    "# hostname = 'localhost'\n",
    "# port = 5432\n",
    "# database = 'grab_db'\n",
    "\n",
    "# db_url = f'postgresql://{username}:{password}@{hostname}:{port}/{database}'\n",
    "\n",
    "# #create a connection to the database\n",
    "# #engine = create_engine('postgresql://postgres:admin@localhost:5432/grab_db')\n",
    "# engine = create_engine(db_url)\n",
    "\n",
    "\n",
    "# # Create a new establishment table\n",
    "# commands = ('''Create TABLE IF NOT EXISTS establishment_tbl(id_source VARCHAR PRIMARY KEY,\n",
    "#                                                 name VARCHAR,\n",
    "#                                                 address VARCHAR,\n",
    "#                                                 lat NUMERIC,\n",
    "#                                                 lon NUMERIC,\n",
    "#                                                 rating NUMERIC,\n",
    "#                                                 reviews_nr NUMERIC,\n",
    "#                                                 loc_type VARCHAR\n",
    "#                                                 );''')\n",
    "\n",
    "# #Copy the data to the PostgreSQL table\n",
    "# establishment_df.to_sql('establishment_tbl', engine, if_exists='replace',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataframe : cuisine_df  - HASSAN\n",
    "\n",
    "first we are going to replace the establishment_df['cuisine'] list rows from \" into ' quote.\n",
    "Then we create a column for the eventual primary key called cus_id\n",
    "then we only have 3 columns in cuisine_df which is cuisine_id , id_source and cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transferring master_df to cuisines_df\n",
    "\n",
    "cuisine_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transferring master_df to cuisines_df\n",
    "# cuisine_df=master_df\n",
    "# cuisine_df.head()\n",
    "\n",
    "#clearing ' due to aprostophe 's from cuisine_df['cuisine'] list\n",
    "cuisine_df['cuisine']=cuisine_df['cuisine'].str.replace('\\'',\"\")\n",
    "\n",
    "#clearing \" from cuisine_df['cuisine'] list\n",
    "cuisine_df['cuisine']=cuisine_df['cuisine'].str.replace('\"',\"\")\n",
    "\n",
    "#clearing [,] from cuisine_df['cuisine'] list\n",
    "cuisine_df['cuisine']=cuisine_df['cuisine'].str.replace('[',\"\")\n",
    "cuisine_df['cuisine']=cuisine_df['cuisine'].str.replace(']',\"\")\n",
    "\n",
    "# Add a primary key column\n",
    "cuisine_df['cuisine_id'] = ['cus_' + str(i + 1) for i in range(len(cuisine_df))]\n",
    "\n",
    "cuisine_df=cuisine_df[['cuisine_id','id_source','cuisine']]\n",
    "\n",
    "#making all text in cuisine column to lowercase\n",
    "cuisine_df.loc[:, 'cuisine']=cuisine_df['cuisine'].str.lower()\n",
    "\n",
    "cuisine_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "null values found in cuisine, this we will fill it with thai for index 2473 and korean, beveragees for indx 15049"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuisine_df[cuisine_df['cuisine'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filling up null values in cuisine column with appropriate category types according to their establishment information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index 2473 filling up null values with relevant values\n",
    "cuisine_df.at[2473, 'cuisine'] = 'thai'\n",
    "\n",
    "# index 15049 filling up null values with relevant values\n",
    "cuisine_df.at[15049, 'cuisine'] = 'beverages, korean'\n",
    "\n",
    "#you may check again if there are any more nulls in cuisine column\n",
    "#cuisine_df[cuisine_df['cuisine'].isna()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each row sort the values in the cuisines category in alphabetical order a-z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the values within each string alphabetically\n",
    "cuisine_df['cuisine'] = cuisine_df['cuisine'].apply(lambda x: ', '.join(sorted(x.split(', '))))\n",
    "\n",
    "\n",
    "cuisine_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After sort alphabetically break into seperate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'cuisine' column by comma and expand into separate columns\n",
    "cuisines_split = cuisine_df['cuisine'].str.split(', ', expand=True)\n",
    "\n",
    "# Rename the new columns\n",
    "cuisines_split.columns = [f'cuisine_{i+1}' for i in range(cuisines_split.shape[1])]\n",
    "\n",
    "# Concatenate the new columns with the original DataFrame and drop the old cuisine column\n",
    "cuisine_df = pd.concat([cuisine_df.drop(columns=['cuisine']), cuisines_split], axis=1)\n",
    "\n",
    "cuisine_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking commonality in the grouping to see if we can clean further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check commonlity\n",
    "\n",
    "# Specify selected columns\n",
    "selected_columns = ['cuisine_1', 'cuisine_2', 'cuisine_3', 'cuisine_4', 'cuisine_5']\n",
    "# Apply value_counts function to each column and calculate commonality\n",
    "commonality = cuisine_df[selected_columns].apply(pd.Series.value_counts, axis=1).fillna(0).sum().astype(int)\n",
    "\n",
    "print(commonality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further cleaning to group certain category like michelin star, alcohol and etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further cleaning to change 2 michelin star to 1 michelin star\n",
    "\n",
    "# Find rows where any of the selected columns contain '2 michelin stars'\n",
    "rows_to_replace = cuisine_df[selected_columns].apply(lambda x: x.str.contains('2 michelin stars')).any(axis=1)\n",
    "# Replace '2 michelin stars' with 'michelin star'\n",
    "cuisine_df.loc[rows_to_replace, selected_columns] = cuisine_df[selected_columns].replace('2 michelin stars', '1 michelin star')\n",
    "\n",
    "# further cleaning to change 1 michelin star to michelin star\n",
    "rows_to_replace = cuisine_df[selected_columns].apply(lambda x: x.str.contains('1 michelin star')).any(axis=1)\n",
    "# Replace '2 michelin stars' with 'michelin star'\n",
    "cuisine_df.loc[rows_to_replace, selected_columns] = cuisine_df[selected_columns].replace('1 michelin star', 'michelin star')\n",
    "\n",
    "# further cleaning to change michelin guide to michelin star\n",
    "rows_to_replace = cuisine_df[selected_columns].apply(lambda x: x.str.contains('michelin guide')).any(axis=1)\n",
    "# Replace '2 michelin stars' with 'michelin star'\n",
    "cuisine_df.loc[rows_to_replace, selected_columns] = cuisine_df[selected_columns].replace('michelin guide', 'michelin star')\n",
    "\n",
    "#further cleanin alcoholic beverages to just alcohol\n",
    "rows_to_replace = cuisine_df[selected_columns].apply(lambda x: x.str.contains('alcoholic beverages')).any(axis=1)\n",
    "# Replace 'alcoholic beverages' with 'alcohol'\n",
    "cuisine_df.loc[rows_to_replace, selected_columns] = cuisine_df[selected_columns].replace('alcoholic beverages', 'alcohol')\n",
    "\n",
    "#to group and count each unique value of cuisine column\n",
    "# Apply value_counts function to each column and calculate commonality\n",
    "commonality = cuisine_df[selected_columns].apply(pd.Series.value_counts, axis=1).fillna(0).sum().astype(int)\n",
    "\n",
    "# Sort commonality counts from biggest to smallest\n",
    "commonality_sort_values = commonality.sort_values(ascending=False)\n",
    "\n",
    "# Sort commonality counts alphabetically\n",
    "commonality_sort_alpha = commonality.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuisine_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding to a Database Grab_df : CUISINE_TBL - HASSAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine, VARCHAR\n",
    "from psycopg2 import sql\n",
    "\n",
    "username='postgres'\n",
    "password = 'admin'\n",
    "hostname = 'localhost'\n",
    "port = '5432'\n",
    "database = 'grab_db'\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=database,\n",
    "    user=username,\n",
    "    password=password,\n",
    "    host=hostname,\n",
    "    port=port\n",
    ")\n",
    "\n",
    "#Create a cursor object to execute SQL queries:\n",
    "cur = conn.cursor()\n",
    "\n",
    "# # Create a new cuisine table\n",
    "commands_has = ('''Create TABLE IF NOT EXISTS cuisine_tbl(cuisine_id VARCHAR PRIMARY KEY,\n",
    "                                                id_source VARCHAR,\n",
    "                                                cuisine_1 VARCHAR,\n",
    "                                                cuisine_2 VARCHAR,\n",
    "                                                cuisine_3 VARCHAR,\n",
    "                                                cuisine_4 VARCHAR,\n",
    "                                                cuisine_5 VARCHAR\n",
    "                                                );''')\n",
    "\n",
    "cur.execute(commands_has)\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "\n",
    "db_url = f'postgresql://{username}:{password}@{hostname}:{port}/{database}'\n",
    "\n",
    "# #create a connection to the database\n",
    "# #engine = create_engine('postgresql://postgres:admin@localhost:5432/grab_db')\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "# Execute the table creation query\n",
    "# with engine.connect() as conn:\n",
    "#     conn.execute(commands_has)\n",
    "\n",
    "# # Step 9: Copy the data to the PostgreSQL table\n",
    "cuisine_df.to_sql('cuisine_tbl', engine, if_exists='append', index=False, dtype={\n",
    "    'cuisine_id': VARCHAR(length=255),  # Define the data type for cuisine_id explicitly\n",
    "    'id_source': VARCHAR(length=255),\n",
    "    'cuisine_1': VARCHAR(length=255),\n",
    "    'cuisine_2': VARCHAR(length=255),\n",
    "    'cuisine_3': VARCHAR(length=255),\n",
    "    'cuisine_4': VARCHAR(length=255),\n",
    "    'cuisine_5': VARCHAR(length=255)\n",
    "\n",
    "} )                        \n",
    "                  \n",
    "cur.close()\n",
    "conn.close() \n",
    "                  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### establishment opening hours dataframe - LEE HONG ENG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c318198a",
   "metadata": {},
   "source": [
    "### Creating grab_df and subsetting opening hours column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc393fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# transferring master_df into grab_df - at the initial i have transferred from master\n",
    "#grab_df = master_df\n",
    "\n",
    "# subset openinghour df with records being dictionaries\n",
    "op_hours_dict_df = grab_df[[\"opening_hours\"]]\n",
    "\n",
    "op_hours_dict_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b405c8dc",
   "metadata": {},
   "source": [
    "### Cleaning opening_hours (JSON objects/ dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7dbc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing true to \"True\"\n",
    "op_hours_dict_df = op_hours_dict_df['opening_hours'].str.replace('true','\"True\"')\n",
    "op_hours_dict_df = pd.DataFrame(op_hours_dict_df)\n",
    "\n",
    "op_hours_dict_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40366199",
   "metadata": {},
   "source": [
    "### Expanding the dictionaries into 10 separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755a01d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from pandas import json_normalize\n",
    "\n",
    "# Ensure that the column is in a proper list of dictionaries format\n",
    "op_hours_dict_df['opening_hours'] = op_hours_dict_df['opening_hours'].apply(ast.literal_eval)\n",
    "\n",
    "# Use json_normalize to convert the dictionaries into separate columns\n",
    "op_hours_dict_df_expanded = op_hours_dict_df.join(json_normalize(op_hours_dict_df['opening_hours'])).drop('opening_hours', axis=1)\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "display(op_hours_dict_df_expanded.head)\n",
    "display(op_hours_dict_df_expanded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5278018",
   "metadata": {},
   "source": [
    "#### Joining id_source column from grab_df to op_hours_dict_df_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e278f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining id_source column from grab_df to op_hours_dict_df_expanded\n",
    "openinghour_df = pd.concat([grab_df[[\"id_source\"]], op_hours_dict_df_expanded], axis=1, sort=False)\n",
    "                  \n",
    "openinghour_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4bd25a",
   "metadata": {},
   "source": [
    "#### Adding index column with prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1f30d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new index column and add a prefix\n",
    "openinghour_df['openinghour_id'] = 'op_' + (openinghour_df.index + 1).astype(str)\n",
    "\n",
    "# Set new index column to be the first column\n",
    "openinghour_df = openinghour_df.set_index('openinghour_id').reset_index()\n",
    "\n",
    "display(openinghour_df.head())\n",
    "display(openinghour_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ea5863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Count no. of NaN in dataframe\n",
    "nan_count = openinghour_df.isna().sum().sum()\n",
    "\n",
    "print(f'Total number of NaN values in the DataFrame: {nan_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ec5654",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of NaN values for each column\n",
    "nan_count_per_column_df = openinghour_df.isna().sum()\n",
    "\n",
    "nan_count_per_column_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3e44a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. of establishment minus no. of tempClosed equals 'NaN'\n",
    "count_open = 16136-12849\n",
    "print(count_open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8843069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame based on the condition Open=NaN and tempClosed=True\n",
    "opennull_tempclosetrue = openinghour_df[pd.isna(openinghour_df['open']) & (openinghour_df['tempClosed'] == 'True')]\n",
    "\n",
    "print('Total no. of rows of tempClosed establishment = ' + str(len(opennull_tempclosetrue)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dd87e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame based on the condition tempClosed=True\n",
    "tempclosedtrue = openinghour_df[pd.isna(openinghour_df['tempClosed'])]\n",
    "\n",
    "print('Total no. of rows of (tempClosed is NaN) establishment = ' + str(len(tempclosedtrue)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d609e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame based on the condition open=NaN\n",
    "opentrue = openinghour_df[(openinghour_df['open'] == 'True')]\n",
    "\n",
    "print('Total no. of rows of (open == \"True\") establishment = ' + str(len(opentrue)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc139add",
   "metadata": {},
   "source": [
    "### There are 12849 - 7155 = 5694 establishment with  open==NaN and tempClosed==NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59893417",
   "metadata": {},
   "outputs": [],
   "source": [
    "12849-7155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a66f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame based on the condition open=NaN and tempClosed=NaN\n",
    "opennull_tempClosednull = openinghour_df[pd.isna(openinghour_df['open']) & pd.isna(openinghour_df['tempClosed'])]\n",
    "\n",
    "print('Total no. of rows of (open == \"NaN\" and tempClosed == \"NaN\") establishment = ' + str(len(opennull_tempClosednull)))\n",
    "opennull_tempClosednull.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd9e515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsetting grab_df with address and id_source\n",
    "grab_df[[\"address\",\"id_source\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inner join on id_source and address to add address column to opennull_tempClosednull dataframe using .merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847045ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing an inner join on the 'id_source' column to include 'address' column to opennull_tempClosednull\n",
    "#syntax for .merge >> inner_joined_df = df1.merge(df2, on='key', how='inner')\n",
    "\n",
    "opennull_tempClosednull_address = grab_df[[\"address\",\"id_source\"]].merge(opennull_tempClosednull, on='id_source', how='inner')\n",
    "opennull_tempClosednull_address.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec359e4",
   "metadata": {},
   "source": [
    "<span style=\"color: blue; font-size: 20px;\">Based on checking on first 5 establishments at time of interim project, they are currently open, meaning they were not permanently closed at time of dataset. Therefore, for sake of analysis in our project, it is assumed that all entries with open=NaN and tempClosed=NaN are not closed permanently, and are therefore included in the analysis of the dataset..</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da69362",
   "metadata": {},
   "source": [
    "#### Replacing open=NaN to True and tempClosed=NaN to False for rows with open=NaN & tempClosed=NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756074ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditionally fill NaN values in open with 'True' and in tempClosed with 'False'\n",
    "openinghour_df.loc[openinghour_df['open'].isna() & openinghour_df['tempClosed'].isna(), ['open', 'tempClosed']] = [True, False]\n",
    "\n",
    "#to check filling of NaN values for open=NaN and tempClosed=NaN \n",
    "openinghour_df.iloc[351]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba50030f",
   "metadata": {},
   "source": [
    "#### For open=true and tempClosed=NaN, replace tempClosed=NaN to False\n",
    "#### For open=NaN and tempClosed=true, replace open=NaN to False\n",
    "#### For any of the days not = 'Closed', replace open to True and tempClosed to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8dcdf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Conditionally fill NaN values in tempClosed with 'False'\n",
    "openinghour_df.loc[openinghour_df['tempClosed'].isna(), ['tempClosed']] = ['False']\n",
    "\n",
    "# Conditionally fill NaN values in open with 'False'\n",
    "openinghour_df.loc[openinghour_df['open'].isna(), ['open']] = ['False']\n",
    "\n",
    "# Update 'open' to True and 'tempClosed' to False where any of the days are not = 'Closed'\n",
    "openinghour_df.loc[(openinghour_df['displayedHours'] != 'closed') \n",
    "                     | (openinghour_df['sun'] != 'Closed') \n",
    "                     | (openinghour_df['mon'] != 'Closed')\n",
    "                     | (openinghour_df['tue'] != 'Closed') \n",
    "                     | (openinghour_df['wed'] != 'Closed')\n",
    "                     | (openinghour_df['thu'] != 'Closed') \n",
    "                     | (openinghour_df['fri'] != 'Closed')\n",
    "                     | (openinghour_df['sat'] != 'Closed') \n",
    "                     , ['open', 'tempClosed']] = [True, False]\n",
    "\n",
    "openinghour_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e424c0",
   "metadata": {},
   "source": [
    "#### 9 establishment where displayedHours and all days are = 'Closed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285f0954",
   "metadata": {},
   "source": [
    "#### should we drop this 9 rows? some are mix n match, some arenot closed on google. MAYBE, they not using grab anymore? - will leave it til joining with establishment_tbl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f50660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying rows where displayedHours and all days are = 'Closed'\n",
    "all_closed = openinghour_df[(openinghour_df['displayedHours'] == 'Closed')\n",
    "                             & (openinghour_df['sun'] == 'Closed') \n",
    "                             & (openinghour_df['mon'] == 'Closed')\n",
    "                             & (openinghour_df['tue'] == 'Closed') \n",
    "                             & (openinghour_df['wed'] == 'Closed')\n",
    "                             & (openinghour_df['thu'] == 'Closed') \n",
    "                             & (openinghour_df['fri'] == 'Closed')\n",
    "                             & (openinghour_df['sat'] == 'Closed')] \n",
    "                     \n",
    "# Update 'open' to False and 'tempClosed' to True where displayedHours and all days are = 'Closed'\n",
    "print(len(all_closed))\n",
    "\n",
    "all_closed_address = grab_df[[\"address\",\"id_source\"]].merge(all_closed, on='id_source', how='inner')\n",
    "all_closed_address.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openinghour_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(openinghour_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING table in database GRAB_DB: openinghour - LEE HONG ENG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559ca30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "username='postgres'\n",
    "password = 'admin'\n",
    "hostname = 'localhost'\n",
    "port = 5432\n",
    "database = 'grab_db'\n",
    "\n",
    "db_url = f'postgresql://{username}:{password}@{hostname}:{port}/{database}'\n",
    "\n",
    "# Create engine\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "# Creating connection to grab_db - amend the password accordingly\n",
    "#engine = create_engine('postgresql://postgres:interim@localhost:5432/grab_db')\n",
    "\n",
    "# Send dataframe to database as a table\n",
    "openinghour_df.to_sql('openinghour', engine, if_exists='replace', index=False)\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    # Define the SQL statement using SQLAlchemy text construct\n",
    "    sql_statement = text(\"\"\"\n",
    "        ALTER TABLE openinghour\n",
    "        ADD CONSTRAINT openinghour_id PRIMARY KEY (openinghour_id);\n",
    "        \"\"\")\n",
    "        \n",
    "        # Execute the SQL statement\n",
    "    connection.execute(sql_statement)\n",
    "        \n",
    "    # Commit the transaction\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROSNI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transforming data to put in delivery_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fdc2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_rest.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Details of delivery_df:(ROSNI)\n",
    "#id_source,delivery_cost, radius, delivery_options, promo \n",
    "delivery = grab_rest[['id_source','delivery_cost','radius','delivery_options','promo']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listing the name of columns in delivery data frame\n",
    "delivery.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copying all the contents of the table \"delivery\" and assigning the name \"delivery_1\" \n",
    "delivery_1=delivery.copy()\n",
    "#to delivery_1 adding extra column named delivery_cost_sds with value =delivery_cost/100\n",
    "delivery_1['delivery_cost_sds']=delivery['delivery_cost']/100\n",
    "delivery_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting only the required columns to make a new df\n",
    "delivery_df= delivery_1[['id_source','delivery_cost_sds','radius','delivery_options','promo']]\n",
    "delivery_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##delivery_promo = delivery_df['promo']\n",
    "#delivery_promo['promo'].fillna (\"No promotion\", inplace= True)\n",
    "# this code didnt work. so trying again\n",
    "# for thiscode delivery_promo =delivery_df[['promo']]\n",
    "#error message---C:\\Users\\Ros\\AppData\\Local\\Temp\\ipykernel_31724\\4246233546.py:7: SettingWithCopyWarning: \n",
    "#A value is trying to be set on a copy of a slice from a DataFrame\n",
    "#See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
    "\n",
    "#so i converted the delivery_promo table to df\n",
    "delivery_promo = pd.DataFrame({'promo': delivery_df['promo']})\n",
    "\n",
    "delivery_promo['promo'].fillna(\"No Promotion available\", inplace= True)\n",
    "delivery_promo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delivery_promo = pd.DataFrame({'promo': delivery_promo['promo']}): This line creates a new df by calling the pd.DataFrame() constructor. It takes a dictionary as an argument, where the keys are the column names and the values are the column data. In this case, the dictionary has a single key-value pair where the key is 'promo' (the name of the column) and the value is delivery_promo['promo']. This extracts the 'promo' column from the existing DataFrame delivery_promo and creates a new DataFrame with a single column named 'promo'.Essentially, this line of code is used to isolate the 'promo' column from the original DataFrame and create a new DataFrame with just that column. This step may be necessary to perform operations specifically on the 'promo' column without affecting the original DataFrame structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Using .loc to modify the original DataFrame without triggering the warning\n",
    "delivery_df.loc[:, 'promo'] = delivery_promo['promo']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_df.info()\n",
    "#from this we can understand the delivery_cost_sds column has 16085 non-null values.The remaining 51 ROWS are null or NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using isna() method to create a boolean mask for NaN values\n",
    "nan_mask = delivery_df.isna().any(axis=1)\n",
    "\n",
    "# Filter the DataFrame to select rows with NaN values\n",
    "rows_with_nan = delivery_df[nan_mask]\n",
    "\n",
    "# Display the rows with NaN values\n",
    "print(rows_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check how many columns and rows the contain NaN value.51 columns have NaN values under delivery_cost_sds\n",
    "rows_with_nan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grab_rest_name=grab_rest [['id_source','name']]\n",
    "grab_rest_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using isna() method to create a boolean mask for NaN values\n",
    "nan_mask_main = grab_rest_name.isna().any(axis=1)\n",
    "\n",
    "# Filter the DataFrame to select rows with NaN values\n",
    "names_with_nan = grab_rest_name[nan_mask_main]\n",
    "\n",
    "# Display the rows with NaN values\n",
    "print(names_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of values to delete\n",
    "rows_to_delete = ['4-CZDJJTWFC8BAAN','4-C2T2GYAJBEVFJN','4-CZLVCKMHNYTXNX','4-CZKKR7K2JJ22NT', '4-CYNANF42AEVAVA','4-CZEXEPEWN6KKEN']  \n",
    "\n",
    "# Filtering the DataFrame to exclude rows with specified values in the 'id_source' column\n",
    "delivery_df = delivery_df[~delivery_df['id_source'].isin(rows_to_delete)]\n",
    "\n",
    "# Alternatively, if you want to drop rows directly based on the specified values\n",
    "# delivery_df = delivery_df.drop(delivery_df[delivery_df['id_source'].isin(values_to_delete)].index)\n",
    "\n",
    "# Verify the DataFrame after deletion\n",
    "delivery_df #now after 6 rows got deleted,now we have only 16130 rows and 5 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_df.iloc[650:705] #just checking if the rows are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique_identifiers\n",
    "delivery_df ['delivery_options'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_options_df =delivery_df[['delivery_options']]\n",
    "delivery_options_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns with default False values\n",
    "#delivery_options_df['delivery'] = False\n",
    "#delivery_options_df['take_away'] = False\n",
    "#delivery_options_df['dine_in'] = False\n",
    "\n",
    "# Update values based on conditions\n",
    "#delivery_options_df.loc[delivery_options_df['delivery_options'].str.contains('delivery', case=False), 'delivery'] = True\n",
    "#delivery_options_df.loc[delivery_options_df['delivery_options'].str.contains('take', case=False), 'take_away'] = True\n",
    "#delivery_options_df.loc[delivery_options_df['delivery_options'].str.contains('dine', case=False), 'dine_in'] = True\n",
    "\n",
    "# Display the modified DataFrame\n",
    "delivery_options_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns with default False values\n",
    "delivery_df['delivery'] = False\n",
    "delivery_df['take_away'] = False\n",
    "delivery_df['dine_in'] = False\n",
    "\n",
    "# Update values based on conditions\n",
    "delivery_df.loc[delivery_df['delivery_options'].str.contains('delivery', case=False), 'delivery'] = True\n",
    "#For the 'delivery' column, it checks if the 'delivery_options' column contains the word 'delivery' (case insensitive), and if so, sets the corresponding value in the 'delivery' column to True.\n",
    "delivery_df.loc[delivery_df['delivery_options'].str.contains('take', case=False), 'take_away'] = True\n",
    "# For the 'take_away' column, it checks if the 'delivery_options' column contains the word 'take' (case insensitive), and if so, sets the corresponding value in the 'take_away' column to True.\n",
    "delivery_df.loc[delivery_df['delivery_options'].str.contains('dine', case=False), 'dine_in'] = True\n",
    "\n",
    "# Display the modified DataFrame\n",
    "delivery_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding new index column ,'delivery_id'.\n",
    "delivery_df['delivery_id']= 'delv_'+(delivery_df.index+1).astype(str)\n",
    "\n",
    "delivery_df= delivery_df.set_index ('delivery_id').reset_index()\n",
    "\n",
    "delivery_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_df.iloc[5850:5862]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading table to database grab_db: delivery_tbl - ROSNI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine, VARCHAR, NUMERIC, BOOLEAN\n",
    "from psycopg2 import sql\n",
    "\n",
    "username='postgres'\n",
    "password = 'admin'\n",
    "hostname = 'localhost'\n",
    "port = '5432'\n",
    "database = 'grab_db'\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=database,\n",
    "    user=username,\n",
    "    password=password,\n",
    "    host=hostname,\n",
    "    port=port\n",
    ")\n",
    "\n",
    "#Create a cursor object to execute SQL queries:\n",
    "cur = conn.cursor()\n",
    "\n",
    "# # Create a new delivery table\n",
    "commands_rosni = ('''CREATE TABLE IF NOT EXISTS delivery_tbl (\n",
    "    delivery_id VARCHAR PRIMARY KEY,\n",
    "    id_source VARCHAR,\n",
    "    delivery_cost_sds NUMERIC,\n",
    "    radius NUMERIC,\n",
    "    delivery_options VARCHAR,\n",
    "    promo VARCHAR,\n",
    "    delivery BOOLEAN,\n",
    "    take_away BOOLEAN,\n",
    "    dine_in BOOLEAN\n",
    ");''')\n",
    "\n",
    "cur.execute(commands_rosni)\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "\n",
    "db_url = f'postgresql://{username}:{password}@{hostname}:{port}/{database}'\n",
    "\n",
    "# #create a connection to the database\n",
    "# #engine = create_engine('postgresql://postgres: rosni@localhost:5432/grab_db')\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "# Execute the table creation query\n",
    "# with engine.connect() as conn:\n",
    "#     conn.execute(commands_rosni)\n",
    "\n",
    "# # Step 9: Copy the data to the PostgreSQL table\n",
    "delivery_df.to_sql('delivery_tbl', engine, if_exists='append', index=False, dtype={\n",
    "    'delivery_id': VARCHAR(length=255),  # Define the data type for delivery_id explicitly\n",
    "    'id_source': VARCHAR(length=255),\n",
    "    'delivery_cost_sds': NUMERIC,\n",
    "    'delivery_options': VARCHAR(length=255),\n",
    "    'promo': VARCHAR,\n",
    "    'delivery': BOOLEAN,\n",
    "    'take_away': BOOLEAN,\n",
    "    'dine_in': BOOLEAN\n",
    "\n",
    "} )                        \n",
    "                  \n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy import create_engine\n",
    "# username='postgres'\n",
    "# password = 'admin'\n",
    "# hostname = 'localhost'\n",
    "# port = 5432\n",
    "# database = 'grab_db'\n",
    "\n",
    "# conn = psycopg2.connect(\n",
    "#     dbname=database,\n",
    "#     user=username,\n",
    "#     password=password,\n",
    "#     host=hostname,\n",
    "#     port=port\n",
    "# )\n",
    "\n",
    "# #Create a cursor object to execute SQL queries:\n",
    "# cur = conn.cursor()\n",
    "\n",
    "\n",
    "# # Create a new delivery table\n",
    "# commands_rosni = ('''Create TABLE IF NOT EXISTS delivery_tbl(delivery_id VARCHAR PRIMARY KEY,\n",
    "#                                                 id_source VARCHAR,\n",
    "#                                                 delivery_cost_sds VARCHAR,\n",
    "#                                                 radius int,\n",
    "#                                                 delivery_options VARCHAR,\n",
    "#                                                 promo  VARCHAR,\n",
    "#                                                 delivery BOOLEAN,\n",
    "#                                                 take_away BOOLEAN,\n",
    "#                                                 dine_in BOOLEAN\n",
    "#                                                 );''')\n",
    "\n",
    "\n",
    "# cur.execute(commands_rosni)\n",
    "# conn.commit()\n",
    "\n",
    "\n",
    "# connection_uri = f'postgresql://{username}:{password}@{hostname}:{port}/{database}'\n",
    "\n",
    "# #connection_uri ='postgresql://postgres:rosni@localhost:5432/grab_db' #uri for connecting to postgrsql\n",
    "# engine = create_engine (connection_uri) #engine for connecting \n",
    "# delivery_df.to_sql('delivery_tbl',engine, if_exists='replace',index=False) #copying the delivery_df to delivery_tbl\n",
    "\n",
    "# cur.close()\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding *establishment_tbl* to Grab table after adding the 3 foreign key: cuisine_df['cuisine_id'], openinghour_df['openinghour_id'], delivery_df['delivery_id'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "establishment_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establishment_df2=establishment_df.copy()\n",
    "\n",
    "#establishment_df2.info()\n",
    "\n",
    "#merging 1 column cuisine_df['cuisine_id'] based on id _source\n",
    "establishment_df=pd.merge(establishment_df,cuisine_df[['id_source','cuisine_id']],on='id_source')\n",
    "\n",
    "#merging 1 column  openinghour_df['openinghour_id'] based on id _source\n",
    "establishment_df=pd.merge(establishment_df,openinghour_df[['id_source','openinghour_id']],on='id_source')\n",
    "\n",
    "#merging 1 column  delivery_df['delivery_id'] based on id _source\n",
    "establishment_df=pd.merge(establishment_df,delivery_df[['id_source','delivery_id']],on='id_source')\n",
    "\n",
    "establishment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sqlalchemy_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "from sqlalchemy_utils import create_database\n",
    "from sqlalchemy import create_engine\n",
    "username='postgres'\n",
    "password = 'admin'\n",
    "hostname = 'localhost'\n",
    "port = 5432\n",
    "database = 'grab_db'\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=database,\n",
    "    user=username,\n",
    "    password=password,\n",
    "    host=hostname,\n",
    "    port=port\n",
    ")\n",
    "\n",
    "#Create a cursor object to execute SQL queries:\n",
    "cur = conn.cursor()\n",
    "\n",
    "\n",
    "# Create a new establishment table\n",
    "commands_saila = ('''Create TABLE IF NOT EXISTS establishment_tbl(id_source VARCHAR PRIMARY KEY,\n",
    "                                                cuisine_id VARCHAR REFERENCES cuisine_tbl(cuisine_id),\n",
    "                                                openinghour_id VARCHAR REFERENCES openinghour(openinghour_id),\n",
    "                                                delivery_id VARCHAR REFERENCES delivery_tbl(delivery_id),\n",
    "                                                name VARCHAR,\n",
    "                                                address VARCHAR,\n",
    "                                                lat NUMERIC,\n",
    "                                                lon NUMERIC,\n",
    "                                                rating NUMERIC,\n",
    "                                                reviews_nr NUMERIC,\n",
    "                                                loc_type VARCHAR\n",
    "                                                );''')\n",
    "\n",
    "\n",
    "cur.execute(commands_saila)\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "db_url = f'postgresql://{username}:{password}@{hostname}:{port}/{database}'\n",
    "\n",
    "#create a connection to the database\n",
    "#engine = create_engine('postgresql://postgres:admin@localhost:5432/grab_db')\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "# # Step 9: Copy the data to the PostgreSQL table\n",
    "establishment_df.to_sql('establishment_tbl', engine, if_exists='append', index=False, dtype={\n",
    "    'id_source': VARCHAR(length=255),  # Define the data type for delivery_id explicitly\n",
    "    'cuisine_id': VARCHAR(length=255),\n",
    "    'openinghour_id': VARCHAR(length=255),\n",
    "    'delivery_id': VARCHAR(length=255),\n",
    "    'name': VARCHAR,\n",
    "    'address': VARCHAR,\n",
    "    'lat': NUMERIC,\n",
    "    'lon': NUMERIC,\n",
    "    'rating': NUMERIC,\n",
    "    'reviews_nr': NUMERIC,\n",
    "    'loc_type': VARCHAR,\n",
    "\n",
    "\n",
    "} )                        \n",
    "                  \n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "#Copy the data to the PostgreSQL table\n",
    "# establishment_df.to_sql('establishment_tbl2', engine, if_exists='replace',index=False)\n",
    "\n",
    "\n",
    "# cur.close()\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
